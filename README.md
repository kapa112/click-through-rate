# Background

Part of this project was done in a group of 3 students during my master studies at the University of Amsterdam. It implements Naive Bayes and Logistic Regression with Stochastic Gradient Descent for click-through rate project from Kaggle: [Click-Through Rate Prediction](https://www.kaggle.com/c/avazu-ctr-prediction/overview). The focus in this project is on implementing both algorithms from scratch (without using any ML packages) and using sparse matrices from [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html). File `data_transformation.ipynb` selects the features and one-hot encodes the data in the sparse format. File `analysis.ipynb` implements the abovementioned algorithms.

In this project, I did not perform data cleaning and EDA, which could potentially improve the results. Major improvements could also be made by tuning the learning rate / batches in Logistic Regression and using Cross-Validation. Feedback is welcomed.